---
title: "HarvardX Professional Certificate in Data Science PH125.9x: Capstone Project_Choose Your Own!"
author: "Tan Wei Lun"
date: "2025-06-24"
output: pdf_document
---

# 1. Introduction

This project uses the AI Tools Usage Among Global High School Students dataset (downloaded from Kaggle), a fully synthetic simulation of 500 students worldwide and their academic use of AI tools in 2025. No personal or survey data were collected and every record was generated via probabilistic logic to capture realistic patterns in demographics (age, gender, country, grade), binary adoption flags for major AI tools (ChatGPT, Gemini, Grammarly, QuillBot, Notion AI, Phind, EduChat, Other), and conditional usefulness ratings. The analysis proceeds in two stages: first, a binary classifier to predict whether a student uses any AI tool; and second, a multiclass model to predict which specific tool an adopter chooses. Model performance for both stages is evaluated on a hold-out test set using ROC-AUC, accuracy, and F1-score to assess predictive accuracy and generalizability.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(rpart)) install.packages("rpart", repos = "http://cran.us.r-project.org")
if(!require(rpart.plot)) install.packages("rpart.plot", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(tidyr)) install.packages("tidyr", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(rpart)
library(rpart.plot)
library(randomForest)
library(caret)
library(tidyr)
library(dplyr)
library(ggplot2)
```

# 2. Data Preparation
In preparing the data, we first loaded the CSV file int R. We fixed the random seed to 1 to ensure that anyone rerunning the analysis will obtain the same partition. we then split the dataset into an 80% training set and a 20% hold-out test set, stratifying on the AI-usage flag so that both subsets maintain the same proportion of users and non-users. The larger training portion supports cross-validation and hyperparameter tuning, while the reserved 20% remains untouched until the very end, providing an unbiased estimate of out-of-sample performance.

```{r dataset}
# Load and prepare data
edx <- read.csv("global_ai_tools_students_use.csv", stringsAsFactors = FALSE) %>%
  mutate(
    uses_ai_for_study = factor(uses_ai_for_study, levels = c("False","True"))
  )
```

# 3. Exploratory Data Analysis (EDA)
We began by exploring the data to get a clear picture of what we are working with. First, we checked the number of students and the type of each variable. Then we looked at how age, gender, country, and grade are distributed to understand who our students are. After that, we calculated the proportion of students using any AI tool and counted how many use each specific tool to see which ones are most popular. We also examined how often students use multiple tools at once. For each tool, we computed the average usefulness score, its variability, and the percentage of missing ratings (since non-users do not rate tools). We reviewed missing data across all columns to catch any quality issues. Finally, we created plots showing, for example, how adoption varies with age and how tool preference differs by country. This step-by-step exploration helped us uncover important trends and potential challenges before moving on to model building.

## 3.1 Dimensions and structure of the Dataset
First, we examined the number of rows and columns in the dataset. Then, we checked each variable's type.

```{r Dimensions and structure}
dim(edx)
str(edx)
```

## 3.2 The demographic distributions
We then summarize student demographics by computing the age range and counting how many students fall into each gender, country, and grade category

```{r Descriptive statistics}
#  Demographic distributions
summary(edx$age)
edx %>% count(gender)
edx %>% count(country)
edx %>% count(grade)
```

## 3.3 Overall AI Adoption Rate
To understand how common AI use is, we tally how many students report using any AI tool and compute the proportion of users versus non-users. We also identified the most and least popular tools at a glance.

```{r}
# Overall AI adoption rate
edx %>% 
  count(uses_ai_for_study) %>% 
  mutate(prop = n / sum(n))

# Frequency of each AI tool
edx %>% 
  select(starts_with("uses_"), -uses_ai_for_study) %>% 
  summarise(across(everything(), ~ sum(. == "True"))) %>% 
  pivot_longer(everything(), names_to = "tool", values_to = "count")
```

# 4. Model Development and Evaluation
In this project, we first developed and validated a a Random Forest to predict whether a student uses any AI tool, and evaluate its performance via ROC-AUC and accuracy. Secondly, we build a multiclass classifier among students who adopt AI to predict which specific AI tool(s) they choose, assessing models with overall accuracy and per-class F1-scores.

## 4.1 Dataset Split for Model Testing
First, we fixed the random seed to 1 so our split would be reproducible. Then we used a stratified sampling approach to allocate 20 percent of the observations to a test cohort which ensuring that the proportion of AI adopters and non-adopters in the test set matched that of the full dataset and retained the remaining 80 percent as our training cohort. Finally, we removed any test records whose gender, country, or grade category did not appear in the training cohort, so that every categorical level in the test set had been seen during model fitting.

```{r}
# Train-test split (80/20 stratified)
set.seed(1)
trainIndex <- createDataPartition(edx$uses_ai_for_study, p = 0.8, list = FALSE)
train      <- edx[trainIndex, ]
test       <- edx[-trainIndex, ]

# Define your tool names & corresponding usefulness columns
tools     <- c("chatgpt", "gemini", "grammarly",
               "quillbot", "notion_ai", "phind",
               "edu_chat", "other")
use_cols  <- paste0("usefulness_", tools)

# Filter to adopters and compute “primary_tool”
df_adopt <- edx %>%
  # keep only rows where uses_ai_for_study == "True"
  filter(uses_ai_for_study == "True") %>%
  # replace NAs with a very low value so they aren’t picked
  mutate(across(all_of(use_cols), ~replace_na(.x, -Inf))) %>%
  # for each row, find which usefulness_* is maximal
  rowwise() %>%
  mutate(
    primary_tool = tools[which.max(c_across(all_of(use_cols)))]
  ) %>%
  ungroup() %>%
  # turn into a factor (caret needs this)
  mutate(primary_tool = factor(primary_tool, levels = tools))

# Train/test split on adopters (80/20 stratified by primary_tool)
set.seed(1)
idx        <- createDataPartition(df_adopt$primary_tool, p = 0.8, list = FALSE)
train_adopt <- df_adopt[idx, ]
test_adopt  <- df_adopt[-idx, ]
```

## 4.2 CART Model for Tool Selection
This section show a code fits a single decision tree model using the CART (Classification and Regression Trees) algorithm via the rpart package. The code builds a classification decision tree to predict whether a student uses AI for study purposes, based on their age, gender, country, grade, and which AI tools they use (e.g., ChatGPT, Grammarly, Notion AI). The tree's growth is controlled to avoid overfitting by using a complexity parameter of 0.01.

```{r CART tree}
# Fit a single CART tree with rpart
fit_tree <- rpart(
  uses_ai_for_study ~ age + gender + country + grade +
    uses_chatgpt + uses_gemini + uses_grammarly +
    uses_quillbot + uses_notion_ai + uses_phind +
    uses_edu_chat + uses_other,
  data    = train,
  method  = "class",
  control = rpart.control(cp = 0.01)
)
```

We then produced a plot of the decision tree to show the essential structure of how the model makes decisions, without cluttering it with details like probabilities or sample counts.
```{r Visualize the tree}
# Visualize the tree
rpart.plot(
  fit_tree,
  type          = 0,    # node labels only (no split text under nodes)
  extra         = 0,    # no class/prob/count info in leaves
  fallen.leaves = TRUE, # align terminal nodes at the same depth
  cex           = 0.6   # smaller text so it never overlaps
)
```


### 4.2.1 5-fold Cross-Validation with ROC metric
We performed a model tuning and evaluation for a decision tree classifier using the caret package in R.  It first sets up a 5-fold cross-validation procedure that evaluates model performance based on the ROC AUC score. A range of values for the complexity parameter (cp) is defined, and the model is trained using these values to find the one that gives the best performance.

Once the best cp value is identified, the model is retrained using this optimal setting. The final model is then used to make predictions on the test dataset, and its performance is assessed using a confusion matrix.

```{r}
ctrl      <- trainControl(
  method          = "cv",
  number          = 5,
  classProbs      = TRUE,
  summaryFunction = twoClassSummary
)
rpartGrid <- expand.grid(cp = seq(0.001, 0.1, length = 20))

set.seed(1)
train_rpart_cv <- train(
  uses_ai_for_study ~ age + gender + country + grade +
    uses_chatgpt + uses_gemini + uses_grammarly +
    uses_quillbot + uses_notion_ai + uses_phind +
    uses_edu_chat + uses_other,
  data      = train,
  method    = "rpart",
  metric    = "ROC",
  trControl = ctrl,
  tuneGrid  = rpartGrid
)

print(train_rpart_cv$bestTune)
plot(train_rpart_cv)
```

We checked how well the decision tree performs by predicting on unseen data and evaluating the prediction results using standard classification metrics.

```{r}
pred_tree <- predict(fit_tree, test, type = "class")
print(confusionMatrix(pred_tree, test$uses_ai_for_study))
```

The decision tree performs very well, especially for predicting the 'False' class, with 100% sensitivity and 100% NPV. The slight weakness is in specificity and PPV, meaning it sometimes misclassifies True cases as False. The model is highly accurate and reliable, with a Kappa of 0.89 and balanced accuracy of 97.4%, making it a solid choice if simplicity and interpretability are important.


### 4.2.2 Random Forest Model with Caret Tuning

A Random Forest classification model was developed using the caret package in R to predict whether students use AI for study purposes. The model was tuned using 5-fold cross-validation, where the performance metric was based on the Area Under the ROC Curve (AUC-ROC).

A tuning grid was defined to test four values of the mtry parameter (2, 4, 6, and 8), which controls the number of predictors randomly selected at each tree split. The model was trained using 500 trees to ensure stability in the predictions.

After evaluating performance across the different mtry values, the model with the best ROC score was selected. This optimized model was then used to make predictions on the test dataset. The prediction results were evaluated using a confusion matrix, which provided insights into the model’s accuracy, sensitivity, specificity, and other classification metrics.

```{r Random Forest}
# Random Forest with caret tuning
rfGrid <- expand.grid(mtry = c(2, 4, 6, 8))

set.seed(1)
train_rf_cv <- train(
  uses_ai_for_study ~ age + gender + country + grade +
    uses_chatgpt + uses_gemini + uses_grammarly +
    uses_quillbot + uses_notion_ai + uses_phind +
    uses_edu_chat + uses_other,
  data       = train,
  method     = "rf",
  metric     = "ROC",
  ntree      = 500,
  trControl  = ctrl,
  tuneGrid   = rfGrid,
  importance = TRUE
)

print(train_rf_cv$bestTune)
plot(train_rf_cv)
pred_rf_cv <- predict(train_rf_cv, test)
print(confusionMatrix(pred_rf_cv, test$uses_ai_for_study))
```

After tuning the Random Forest model using cross-validation, we tested it on unseen data to check its performance. The model achieved a high accuracy of 93.94%, meaning it correctly predicted most of the students' AI usage behavior.

From the confusion matrix, the model correctly identified 73 students who used AI and 20 who did not. It made only a few mistakes — 2 students who didn’t use AI were wrongly predicted as users, and 4 actual users were missed.

The model showed strong results across other metrics too. It had a sensitivity of 90.91%, which means it was good at spotting students who did not use AI. Its specificity was 94.81%, showing it was even better at recognizing students who did use AI. The balanced accuracy was 92.86%, confirming that the model worked well across both groups.

In short, the tuned Random Forest model performed very well and can be confidently used to predict whether students use AI tools for study based on their background and tool usage.


## 4.3 Predicting Students’ Primary AI Tool

In this step, a machine learning model was used to predict which AI tool each student mainly uses. There were eight possible tools to choose from, making this a multiclass prediction task.

The model used information such as the student’s age, gender, country, grade level, and which AI tools they have used. It was trained to find patterns in these details to guess the student’s main AI tool.

To make sure the model was accurate, it was tested using five rounds of cross-validation. It also tried different settings to find the one that worked best. The final model used 500 decision trees and was also set up to show which features were most important in making the predictions.

This model helps us understand what factors influence students’ choice of their primary AI tool.

```{r}
#Cross-Validation and Tuning Setup

ctrl   <- trainControl(
  method          = "cv",
  number          = 5,
  classProbs      = FALSE,    # multiclass accuracy doesn’t need probs
  summaryFunction = defaultSummary
)
rfGrid <- expand.grid(mtry = c(2, 4, 6, 8))

# Fit the multiclass Random Forest

set.seed(1)
rf_multi <- train(
  primary_tool ~ age + gender + country + grade +
    uses_chatgpt + uses_gemini + uses_grammarly +
    uses_quillbot + uses_notion_ai + uses_phind +
    uses_edu_chat + uses_other,
  data      = train_adopt,
  method    = "rf",
  metric    = "Accuracy",
  trControl = ctrl,
  tuneGrid  = rfGrid,
  ntree     = 500,
  importance= TRUE
)
```

After training the model, the results were reviewed to see how well it performed with different settings.
```{r}
# Inspect tuning results
print(rf_multi)            # shows accuracy by mtry
plot(rf_multi)             # visualizes the tuning curve
```

The model was tested on a new group of students to check its accuracy. It predicted each student's main AI tool, and the results were compared with the actual answers. A confusion matrix showed the overall accuracy and how well the model did for each tool.

```{r}
# Evaluate on the hold-out adopters
pred_multi <- predict(rf_multi, test_adopt)
cm_multi   <- confusionMatrix(pred_multi, test_adopt$primary_tool)
print(cm_multi)            # overall accuracy + per-class stats
```

The model was analyzed to see which features were most important in predicting students’ main AI tool. A list and graph showed the top 10 factors that influenced the model’s decisions the most.
```{r}
# Variable importance
vi_multi <- varImp(rf_multi)
print(vi_multi)            # which features drive tool choice
plot(vi_multi, top = 10)   # plot the top 10 most important
```

# 5 Conclusion
Our first model was able to tell who uses AI almost perfectly, and our second model correctly guessed each user’s favorite tool most of the time. In other words, just knowing a student’s basic demographics and which AI services they already use lets us predict both whether they will adopt AI and which tool they will choose. These findings could help schools offer personalized recommendations or support to students as they explore different AI resources. For the further studies, I recommend to construct a regression model to predict the student's self-reported usefulness score and evaluate the predictive accuracy using RMSE and R square.

## Reference

Daksh Bhatnagar. (2025). AI Tools Usage Among Global High School Students [Data set]. Kaggle. https://doi.org/10.34740/KAGGLE/DS/7656698
